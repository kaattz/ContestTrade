"""
ResearchContest - ç»Ÿä¸€çš„ç ”ç©¶ä¿¡å·ç«žäº‰ç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. Evaluation: è¯„ä¼°åŽ†å²ä¿¡å·çš„å¸‚åœºè¡¨çŽ°ï¼Œè®¡ç®—reward  
2. Prediction: åŸºäºŽåŽ†å²rewardé¢„æµ‹ä¿¡å·æŽ’åº
3. Selection: é€‰æ‹©ä¼˜è´¨ä¿¡å·ä¸ºæŠ•èµ„æä¾›æƒé‡åˆ†é…
"""

import os
import sys
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta

PROJECT_ROOT = Path(__file__).parent.parent.parent.resolve()
sys.path.append(str(PROJECT_ROOT))

from models.llm_model import GLOBAL_LLM
from utils.market_manager import GLOBAL_MARKET_MANAGER
from config.config import cfg
from research_contest_types import SignalData, ResearchContestResult
from research_data_manager import ResearchDataManager
from research_predictor import ResearchPredictor
from research_weight_optimizer import ResearchWeightOptimizer
from research_signal_judger import ResearchSignalJudger

logger = logging.getLogger(__name__)


class ResearchContest:
    """ç ”ç©¶ä¿¡å·ç«žäº‰ç³»ç»Ÿä¸»æŽ§åˆ¶å™¨"""
    
    def __init__(self, target_agents: List[str] = None):
        self.history_window_days = 5
        self.target_agents = target_agents or []
        self.data_manager = ResearchDataManager(self.history_window_days, PROJECT_ROOT, target_agents)
        self.data_manager.set_market_manager(GLOBAL_MARKET_MANAGER)
        self.predictor = ResearchPredictor(self.history_window_days)
        self.weight_optimizer = ResearchWeightOptimizer(".")
        self.signal_judger = ResearchSignalJudger(str(PROJECT_ROOT / "contest_trade" / "agents_workspace"), self.history_window_days, self.data_manager)
        
        logger.info(f"ResearchConteståˆå§‹åŒ–å®Œæˆ - åŽ†å²çª—å£: {self.history_window_days}å¤©, ç›®æ ‡agents: {len(self.target_agents)}ä¸ª")
    
    async def run_research_contest(self, trigger_time: str, current_signals: Dict[str, SignalData] = None) -> ResearchContestResult:
        logger.info(f"ðŸŽ¯ å¼€å§‹è¿è¡ŒResearchContest - {trigger_time}")
        
        try:
            current_date = trigger_time.split(' ')[0]
            
            # æ­¥éª¤1: åŠ è½½åŽ†å²ä¿¡å·æ•°æ®
            logger.info("æ­¥éª¤1: åŠ è½½åŽ†å²ä¿¡å·æ•°æ®")
            agent_signals = self.data_manager.load_historical_signals(current_date)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_signals = sum(len([s for s in signals_list if s is not None]) for signals_list in agent_signals.values())
            total_evaluated = sum(len([s for s in signals_list if s is not None and s.has_contest_data()]) for signals_list in agent_signals.values())
            logger.info(f"åŠ è½½äº† {total_signals} ä¸ªåŽ†å²ä¿¡å·")
            logger.info(f"å…¶ä¸­ {total_evaluated} ä¸ªå·²æœ‰è¯„ä¼°æ•°æ®ï¼Œ{total_signals - total_evaluated} ä¸ªéœ€è¦è¯„ä¼°")
            
            # æ­¥éª¤2: è¯„ä¼°åŽ†å²ä¿¡å·
            await self._evaluate_missing_signals(agent_signals, current_date)
            
            # æ­¥éª¤3: èŽ·å–å½“å¤©ä¿¡å·çš„judgeè¯„åˆ†
            if not current_signals:
                raise ValueError("é¢„æµ‹æ¨¡åž‹éœ€è¦å½“å¤©ä¿¡å·çš„judgeè¯„åˆ†æ•°æ®ï¼è¯·æä¾›current_signalså‚æ•°ã€‚")
            
            logger.info("æ­¥éª¤3: èŽ·å–å½“å¤©ä¿¡å·judgeè¯„åˆ†")
            current_judge_scores = await self._get_current_judge_scores(current_signals, trigger_time)
            
            if not current_judge_scores:
                raise ValueError("æ— æ³•èŽ·å–å½“å¤©ä¿¡å·çš„judgeè¯„åˆ†ï¼é¢„æµ‹æ¨¡åž‹éœ€è¦12ä¸ªç‰¹å¾ï¼ŒåŒ…æ‹¬7ä¸ªjudgeè¯„åˆ†ç‰¹å¾ã€‚")
            
            # æ­¥éª¤4: é¢„æµ‹æœªæ¥nå¤©å¤æ™®æ¯”çŽ‡
            logger.info("æ­¥éª¤4: é¢„æµ‹æœªæ¥å¤æ™®æ¯”çŽ‡")
            predicted_sharpe_ratios = self._predict_signal_values(current_date, agent_signals, current_judge_scores)
            
            # æ­¥éª¤5: åŸºäºŽé¢„æµ‹å¤æ™®æ¯”çŽ‡åˆ†é…æƒé‡
            logger.info("æ­¥éª¤5: åŸºäºŽé¢„æµ‹å¤æ™®æ¯”çŽ‡åˆ†é…æƒé‡")
            optimized_weights = self.weight_optimizer.optimize_weights_by_sharpe(predicted_sharpe_ratios, trigger_time)
            
            # æ­¥éª¤6: ä¿å­˜ç»“æžœ
            result = self.weight_optimizer.save_final_results_by_sharpe(trigger_time, optimized_weights, predicted_sharpe_ratios)
            
            logger.info(f"âœ… ResearchContestå®Œæˆ: {result.get_summary()}")
            return result
            
        except Exception as e:
            logger.error(f"ResearchContestè¿è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"è¿è¡Œå¤±è´¥: {e}")

    async def run_research_pipeline(self, trigger_time: str, workspace_dir: str = None) -> Dict[str, Any]:
        print(f"ðŸ”¬ å¼€å§‹è¿è¡ŒResearch Contestæµç¨‹ï¼Œæ—¶é—´: {trigger_time}")
        
        try:
            result = await self.run_research_contest(trigger_time)
            
            if result:
                print("âœ… Research Contestæµç¨‹å®Œæˆ")
                print(f"   ä¼˜åŒ–æƒé‡æ•°é‡: {len(result.optimized_weights)}")
                print(f"   æœ‰æ•ˆä¿¡å·æ•°é‡: {result.valid_signals}")
                
                return {
                    'status': 'success',
                    'trigger_time': trigger_time,
                    'optimized_weights': result.optimized_weights,
                    'predicted_sharpe_ratios': result.predicted_sharpe_ratios,
                    'total_signals': result.total_signals,
                    'valid_signals': result.valid_signals,
                    'selection_method': result.selection_method
                }
            else:
                raise RuntimeError("Research Contestæµç¨‹å¤±è´¥: ç»“æžœä¸ºç©º")
                
        except Exception as e:
            raise RuntimeError(f"Research Contestæµç¨‹å¼‚å¸¸: {e}")

    def filter_valid_signals(self, signals_data: Dict[str, SignalData]) -> Dict[str, SignalData]:
        valid_signals = {}
        
        for signal_name, signal_data in signals_data.items():
            has_opportunity = signal_data.has_opportunity
            if has_opportunity.lower() == 'yes':
                valid_signals[signal_name] = signal_data
                print(f"   âœ… ä¿ç•™æœ‰æ•ˆç ”ç©¶ä¿¡å·: {signal_name} ({signal_data.symbol_name})")
            else:
                print(f"   âŒ è¿‡æ»¤æ— æ•ˆç ”ç©¶ä¿¡å·: {signal_name} (has_opportunity={has_opportunity})")
        
        return valid_signals

    def get_signal_details(self, trigger_time: str, signal_names: List[str]) -> Dict[str, Dict]:
        signal_details = {}
        
        current_signals = self.data_manager.load_current_signals(trigger_time)
        
        for signal_name in signal_names:
            if signal_name in current_signals:
                signal = current_signals[signal_name]
                signal_details[signal_name] = {
                    'symbol_name': signal.symbol_name,
                    'action': signal.action,
                    'probability': signal.probability
                }
            else:
                raise ValueError(f"ä¿¡å· {signal_name} ä¸å­˜åœ¨äºŽå½“å‰ä¿¡å·ä¸­")
        
        return signal_details

    def format_signal_output(self, optimized_weights: Dict[str, float], 
                           signal_details: Dict[str, Dict]) -> List[str]:
        output_lines = []

        sorted_weights = sorted(optimized_weights.items(), key=lambda x: x[1], reverse=True)
        
        valid_signals_count = 0
        for signal_name, weight in sorted_weights:
            if weight > 0:
                valid_signals_count += 1
                details = signal_details.get(signal_name, {'symbol_name': 'N/A', 'action': 'N/A', 'probability': 'N/A'})
                symbol_name = details['symbol_name']
                action = details['action']
                probability = details.get('probability', 'N/A')
                output_lines.append(f"   {valid_signals_count}. {symbol_name} - {action} - æ¦‚çŽ‡: {probability} - æƒé‡: {weight:.1%}")
        
        if valid_signals_count == 0:
            output_lines.append("   ðŸ“Š æš‚æ— æœ‰æ•ˆç ”ç©¶ä¿¡å·")
        
        return output_lines

    async def train_prediction_model(self) -> bool:
        try:
            model_dir = Path(__file__).parent / "lightgbm_predictor"
            mean_model_path = model_dir / "lgbm_mean_model.joblib"
            std_model_path = model_dir / "lgbm_std_model.joblib"
            
            if mean_model_path.exists() and std_model_path.exists():
                success = self.predictor._load_lightgbm_models()
                if self.predictor.use_lightgbm:
                    print("âœ… æˆåŠŸå¯¼å…¥çŽ°æœ‰çš„LightGBMæ¨¡åž‹ï¼Œè·³è¿‡è®­ç»ƒ")
                    return True
                else:
                    print("âš ï¸ çŽ°æœ‰æ¨¡åž‹åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è®­ç»ƒ")
            else:
                print("ðŸ” æœªå‘çŽ°çŽ°æœ‰æ¨¡åž‹æ–‡ä»¶ï¼Œéœ€è¦è®­ç»ƒæ–°æ¨¡åž‹")
                if not mean_model_path.exists():
                    print(f"   ç¼ºå¤±æ–‡ä»¶: {mean_model_path}")
                if not std_model_path.exists():
                    print(f"   ç¼ºå¤±æ–‡ä»¶: {std_model_path}")
            
            print("ðŸ¤– å¼€å§‹è®­ç»ƒæ–°çš„Researché¢„æµ‹æ¨¡åž‹...")
            
            training_data = self._collect_historical_training_data()
            
            if not training_data:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
                return False

            success = self.predictor.train_lightgbm_model(training_data)
            
            if success:
                print("âœ… Researché¢„æµ‹æ¨¡åž‹è®­ç»ƒå®Œæˆ")
                print(f"   æ¨¡åž‹å·²ä¿å­˜åˆ°: {model_dir}")
            else:
                print("âŒ Researché¢„æµ‹æ¨¡åž‹è®­ç»ƒå¤±è´¥")
            
            return success
            
        except Exception as e:
            print(f"âŒ Researché¢„æµ‹æ¨¡åž‹è®­ç»ƒ/å¯¼å…¥å¼‚å¸¸: {e}")
            return False

    def get_model_status(self) -> Dict[str, Any]:
        model_dir = Path(__file__).parent / "lightgbm_predictor"
        mean_model_path = model_dir / "lgbm_mean_model.joblib"
        std_model_path = model_dir / "lgbm_std_model.joblib"
        
        status = {
            'model_dir': str(model_dir),
            'mean_model_exists': mean_model_path.exists(),
            'std_model_exists': std_model_path.exists(),
            'models_loaded': self.predictor.use_lightgbm,
            'mean_model_path': str(mean_model_path),
            'std_model_path': str(std_model_path)
        }
        
        if mean_model_path.exists():
            status['mean_model_size'] = os.path.getsize(mean_model_path)
            status['mean_model_modified'] = os.path.getmtime(mean_model_path)
                
        if std_model_path.exists():
            status['std_model_size'] = os.path.getsize(std_model_path)
            status['std_model_modified'] = os.path.getmtime(std_model_path)
        
        return status

    def _collect_historical_training_data(self) -> Dict[str, List]:
        """æ”¶é›†åŽ†å²æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®"""

        print("ðŸ“Š å¼€å§‹æ”¶é›†åŽ†å²è®­ç»ƒæ•°æ®...")
        
        training_data = {}
        current_date = datetime.now()
        valid_days = 0
        
        for days_back in range(180, 0, -1):
            date = current_date - timedelta(days=days_back)
            date_str = date.strftime("%Y-%m-%d")
            
            try:
                day_signals = self.data_manager.load_historical_signals(date_str)
                
                day_has_data = False
                for agent_name, signals_list in day_signals.items():
                    if agent_name not in training_data:
                        training_data[agent_name] = []
                    
                    for signal in signals_list:
                        if signal is not None:
                            training_data[agent_name].append(signal)
                            day_has_data = True
                
                if day_has_data:
                    valid_days += 1
            
            except Exception as e:
                continue
        
        # ç»Ÿè®¡è®­ç»ƒæ•°æ®
        total_samples = sum(len(signals) for signals in training_data.values())
        print(f"ðŸ“ˆ æ”¶é›†äº† {valid_days} å¤©çš„åŽ†å²æ•°æ®")
        print(f"æ€»è®¡ {total_samples} ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¦†ç›– {len(training_data)} ä¸ªagents")
        
        return training_data

    async def _evaluate_missing_signals(self, agent_signals: Dict[str, List[Optional[SignalData]]], current_date: str):
        """è¯„ä¼°ç¼ºå¤±rewardæ•°æ®çš„ä¿¡å·ï¼ˆä½¿ç”¨æ•°æ®ç®¡ç†å™¨è®¡ç®—æ”¶ç›ŠçŽ‡ï¼‰"""
        logger.info("è¯„ä¼°ç¼ºå¤±çš„ä¿¡å·æ•°æ®")
        
        signals_to_evaluate = []
        for agent_name, signals_list in agent_signals.items():
            for signal in signals_list:
                if signal is None:
                    continue
                if not signal.has_contest_data():
                    signal_date = signal.trigger_time.split(' ')[0]
                    signals_to_evaluate.append((signal, signal_date))
        
        if not signals_to_evaluate:
            logger.info("æ‰€æœ‰ä¿¡å·éƒ½å·²æœ‰è¯„ä¼°æ•°æ®ï¼Œè·³è¿‡è¯„ä¼°æ­¥éª¤")
            return
        
        logger.info(f"éœ€è¦è¯„ä¼° {len(signals_to_evaluate)} ä¸ªä¿¡å·")
        
        for signal, signal_date in signals_to_evaluate:
            reward = await self.data_manager.calculate_signal_reward(signal)
            signal.contest_data = {
                'reward': reward,
                'evaluation_date': signal_date,
                'evaluation_method': 'market_return'
            }
        
        logger.info(f"è¯„ä¼°å®Œæˆ: {len(signals_to_evaluate)} ä¸ªä¿¡å·å…¨éƒ¨æˆåŠŸ")

    def _predict_signal_values(self, current_date: str, agent_signals: Dict[str, List[Optional[SignalData]]], 
                             current_judge_scores: Dict[str, List[float]]) -> dict:
        """é¢„æµ‹ä¿¡å·å¾—åˆ†ï¼ˆå¤æ™®æ¯”çŽ‡ï¼‰"""
        logger.info("é¢„æµ‹æœªæ¥å¤æ™®æ¯”çŽ‡")
        
        signal_scores = self.predictor.predict_signal_scores(current_date, agent_signals, current_judge_scores)
        return signal_scores
    
    async def _get_current_judge_scores(self, current_signals: Dict[str, SignalData], 
                                      trigger_time: str) -> Dict[str, List[float]]:
        """èŽ·å–å½“å¤©ä¿¡å·çš„judgeè¯„åˆ†"""
        logger.info(f"èŽ·å–å½“å¤©ä¿¡å·judgeè¯„åˆ† - {len(current_signals)} ä¸ªä¿¡å·")
        
        llm_config = {
            "api_key": cfg.llm.api_key,
            "api_base": cfg.llm.api_base,
            "model_name": cfg.llm.model_name
        }
        
        judge_scores = await self.signal_judger.judge_signals(
            signals=current_signals,
            trigger_time=trigger_time,
            num_judgers=5,
            llm_config=llm_config
        )
        
        logger.info(f"èŽ·å¾— {len(judge_scores)} ä¸ªä¿¡å·çš„judgeè¯„åˆ†")
        return judge_scores


if __name__ == "__main__":
    async def main():
        """æµ‹è¯•å‡½æ•°"""
        research_contest = ResearchContest()
        
        test_time = "2025-08-20 09:00:00"
        result = await research_contest.run_research_contest(test_time)
        
        print("\n" + "="*60)
        print("æµ‹è¯•ç»“æžœ:")
        print(f"æ€»ä¿¡å·æ•°: {result.total_signals}")
        print(f"æœ‰æ•ˆä¿¡å·æ•°: {result.valid_signals}")
        print(f"é€‰æ‹©æ–¹æ³•: {result.selection_method}")
        
        if result.optimized_weights:
            print("\næƒé‡åˆ†é… (Top 5):")
            sorted_weights = sorted(result.optimized_weights.items(), key=lambda x: x[1], reverse=True)
            for i, (signal_name, weight) in enumerate(sorted_weights[:5]):
                if weight > 0:
                    print(f"  {i+1}. {signal_name}: {weight:.1%}")
    
    asyncio.run(main())
