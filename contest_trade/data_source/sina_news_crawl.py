"""
sina news data crawler
"""
import asyncio
import aiohttp
import re
import json
import os
import time
from datetime import datetime
import random
import html
import pandas as pd
import sys
current_dir = os.path.dirname(__file__)
package_root = os.path.dirname(current_dir)
if package_root not in sys.path:
    sys.path.insert(0, package_root)
from data_source.data_source_base import DataSourceBase
from loguru import logger


class SinaNewsCrawl(DataSourceBase):
    def __init__(self, start_page=1, end_page=50):
        super().__init__("sina_news_crawl")
        self.start_page = start_page
        self.end_page = end_page
        # ä½¿ç”¨ä½ æä¾›çš„å®Œæ•´URLæ ¼å¼ï¼Œpage/r/callback å°†åœ¨è¯·æ±‚æ—¶åŠ¨æ€ç”Ÿæˆ
        self.base_url = "http://feed.mix.sina.com.cn/api/roll/get"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
            "Referer": "https://finance.sina.com.cn/",
            "Accept": "*/*",
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
        self.all_items = []
        self.fetch_full_intro = True  # æ˜¯å¦æŠ“å–æ–‡ç« é¡µä»¥è¡¥å…¨ intro
        self.article_concurrency = 2 # æ§åˆ¶æŠ“å–æ–‡ç« é¡µçš„å¹¶å‘
        
    async def fetch_page(self, session, page):
        """å¼‚æ­¥è·å–å•ä¸ªé¡µé¢çš„æ•°æ®"""
        params = {
            "pageid": 384,
            "lid": 2519,
            "k": "",
            "num": 50,
            "page": page
        }
        
        try:
            async with session.get(self.base_url, params=params, headers=self.headers, timeout=15) as response:
                text = await response.text()
                
                # å…¼å®¹ JSONP ä¸ çº¯ JSON
                m = re.search(r'^\s*[\w$]+\((.*)\)\s*;?\s*$', text.strip(), re.S)
                json_text = m.group(1) if m else text.strip()
                data = json.loads(json_text)
                
                # æå–itemsï¼ˆä»…ä¿ç•™æŒ‡å®šå­—æ®µï¼‰
                items = self.extract_items(data, page)

                # å°è¯•è¡¥å…¨ intro
                if self.fetch_full_intro and items:
                    await self.enrich_items_with_full_intro(session, items)

                return items
                
        except Exception as e:
            return []
    
    def extract_items(self, data, page):
        """æå–æ–°é—»itemsï¼Œå¹¶è£å‰ªä¸ºç›®æ ‡å­—æ®µé›†"""
        try:
            if isinstance(data, dict):
                result = data.get("result", {})
                if isinstance(result, dict):
                    data_field = result.get("data", [])
                    if isinstance(data_field, list):
                        processed_items = []
                        for raw in data_field:
                            if not isinstance(raw, dict):
                                continue
                            # é€‰å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ—¶é—´å­—æ®µ
                            candidate_keys = [
                                "ctime", "intime", "mtime", "create_time", "createtime",
                                "pub_time", "pubTime", "pubdate", "pubDate", "time", "update_time"
                            ]
                            raw_time_value = None
                            for key in candidate_keys:
                                if key in raw and raw.get(key) not in (None, ""):
                                    raw_time_value = raw.get(key)
                                    break
                            publish_time = self.normalize_publish_time(raw_time_value)

                            # æœ¬åœ°å¯ç”¨çš„ç®€ä»‹
                            intro_local = self.choose_best_intro_local(raw)
                            # ç›®æ ‡URLï¼ˆä¼˜å…ˆPCï¼Œå…¶æ¬¡WAPï¼Œå…¶æ¬¡urlsæ•°ç»„ï¼‰
                            url = self.choose_best_url(raw)

                            # ä»…ä¿ç•™æŒ‡å®šå­—æ®µ
                            processed_items.append({
                                "title": raw.get("title") or raw.get("stitle") or "",
                                "intro": intro_local or "",
                                "publish_time": publish_time,
                                "media_name": raw.get("media_name") or "",
                                "url": url or "",
                            })
                        return processed_items
            return []
        except Exception as e:
            print(f"ç¬¬ {page} é¡µæ•°æ®è§£æå¤±è´¥: {e}")
            return []
    
    def normalize_publish_time(self, raw_value):
        """å°†å¤šç§æ—¶é—´æ ¼å¼æ ‡å‡†åŒ–ä¸º 'YYYY-MM-DD HH:MM:SS' å­—ç¬¦ä¸²"""
        try:
            if raw_value is None:
                return None
            # æ•°å­—æ—¶é—´æˆ³ï¼ˆç§’æˆ–æ¯«ç§’ï¼‰
            if isinstance(raw_value, (int, float)):
                timestamp = int(raw_value)
            elif isinstance(raw_value, str) and re.fullmatch(r"\d{10,13}", raw_value):
                timestamp = int(raw_value)
            else:
                # å°è¯•è§£æå¸¸è§çš„æ—¶é—´å­—ç¬¦ä¸²
                if isinstance(raw_value, str):
                    for fmt in [
                        "%Y-%m-%d %H:%M:%S",
                        "%Y-%m-%d %H:%M",
                        "%Y-%m-%d",
                        "%Y/%m/%d %H:%M:%S",
                        "%Y/%m/%d %H:%M",
                        "%Y/%m/%d",
                        "%Yå¹´%mæœˆ%dæ—¥ %H:%M",
                        "%Yå¹´%mæœˆ%dæ—¥",
                    ]:
                        try:
                            dt = datetime.strptime(raw_value.strip(), fmt)
                            return dt.strftime("%Y-%m-%d %H:%M:%S")
                        except Exception:
                            pass
                # æ— æ³•è¯†åˆ«åˆ™åŸæ ·è¿”å›å­—ç¬¦ä¸²
                return str(raw_value)

            # æ¯«ç§’ä¸ç§’çš„åŒºåˆ†
            if timestamp > 1_000_000_000_000:
                timestamp //= 1000
            elif 0 < timestamp < 10_000_000_000:
                pass
            else:
                # éå¸¸è§„èŒƒå›´ï¼Œä¿é™©èµ·è§å–å‰10ä½
                timestamp = int(str(timestamp)[:10])

            return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            return str(raw_value)

    def choose_best_url(self, raw_item):
        """é€‰æ‹©æœ€åˆé€‚çš„æ–‡ç« URL"""
        url = raw_item.get("url")
        if url:
            return url
        # æœ‰äº›è¿”å›çš„ urls æ˜¯ JSON å­—ç¬¦ä¸²
        urls_field = raw_item.get("urls")
        if isinstance(urls_field, list) and urls_field:
            return urls_field[0]
        if isinstance(urls_field, str) and urls_field.strip().startswith("["):
            try:
                parsed = json.loads(urls_field)
                if isinstance(parsed, list) and parsed:
                    return parsed[0]
            except Exception:
                pass
        wapurl = raw_item.get("wapurl")
        if wapurl:
            return wapurl
        return None

    def choose_best_intro_local(self, raw_item):
        """åœ¨ä¸è¯·æ±‚æ–‡ç« é¡µçš„æƒ…å†µä¸‹ï¼Œé€‰å–æœ€åˆé€‚çš„ç®€ä»‹å­—æ®µ"""
        candidates = [raw_item.get("intro"), raw_item.get("summary"), raw_item.get("wapsummary")]
        candidates = [c for c in candidates if isinstance(c, str) and c.strip()]
        if not candidates:
            return None
        # é€‰æ‹©æœ€é•¿çš„ä¸€æ¡
        best = max(candidates, key=lambda x: len(x))
        return best

    def should_fetch_full_intro(self, intro_text):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æŠ“å–æ–‡ç« é¡µè¡¥å…¨ç®€ä»‹"""
        if not intro_text:
            return True
        text = intro_text.strip()
        if len(text) < 60:
            return True
        if text.endswith("â€¦") or text.endswith("..."):
            return True
        return False

    async def enrich_items_with_full_intro(self, session, items):
        """å¹¶å‘æŠ“å–æ–‡ç« é¡µï¼Œè¡¥å…¨ intro"""
        semaphore = asyncio.Semaphore(self.article_concurrency)

        async def process_one(item):
            if not self.should_fetch_full_intro(item.get("intro")):
                return
            url = item.get("url")
            if not url:
                return
            try:
                async with semaphore:
                    intro_full = await self.fetch_article_intro(session, url)
                if intro_full and len(intro_full) > len(item.get("intro") or ""):
                    item["intro"] = intro_full
            except Exception:
                pass

        await asyncio.gather(*[process_one(it) for it in items])

    async def fetch_article_intro(self, session, url):
        """æŠ“å–æ–‡ç« é¡µç®€ä»‹ï¼šä¼˜å…ˆ meta description / og:descriptionï¼Œå…¶æ¬¡æ­£æ–‡é¦–æ®µ"""
        try:
            async with session.get(url, headers=self.headers, timeout=15) as resp:
                html_text = await resp.text(errors="ignore")
            if not html_text:
                return None
            # å…ˆå°è¯• meta description / og:description
            meta_desc = self._extract_meta_description(html_text)
            if meta_desc:
                return meta_desc
            # é€€åŒ–åˆ°æ­£æ–‡é¦–æ®µ
            first_paragraph = self._extract_first_paragraph(html_text)
            if first_paragraph:
                return first_paragraph
            return None
        except Exception:
            return None

    def _extract_meta_description(self, html_text):
        """ä»HTMLä¸­æå–<meta name="description">æˆ–<meta property="og:description">"""
        try:
            # name=description
            m1 = re.search(r'<meta[^>]+name=["\']description["\'][^>]+content=["\'](.*?)["\']', html_text, re.I | re.S)
            if m1:
                return html.unescape(self._clean_whitespace(m1.group(1)))
            # property=og:description
            m2 = re.search(r'<meta[^>]+property=["\']og:description["\'][^>]+content=["\'](.*?)["\']', html_text, re.I | re.S)
            if m2:
                return html.unescape(self._clean_whitespace(m2.group(1)))
            return None
        except Exception:
            return None

    def _extract_first_paragraph(self, html_text):
        """ä»å¸¸è§å®¹å™¨ä¸­æå–é¦–æ®µæ–‡æœ¬ï¼ˆç®€æ˜“æ­£åˆ™ç‰ˆï¼‰"""
        try:
            # å¸¸è§æ­£æ–‡å®¹å™¨ id/class: artibody, article, content
            container_patterns = [
                r'<div[^>]+id=["\']artibody["\'][^>]*>(.*?)</div>',
                r'<article[^>]*>(.*?)</article>',
                r'<div[^>]+class=["\'][^"\']*(?:article|content)[^"\']*["\'][^>]*>(.*?)</div>',
            ]
            for pat in container_patterns:
                m = re.search(pat, html_text, re.I | re.S)
                if m:
                    inner = m.group(1)
                    # æ‰¾ç¬¬ä¸€ä¸ª<p>
                    p = re.search(r'<p[^>]*>(.*?)</p>', inner, re.I | re.S)
                    if p:
                        text = self._strip_html_tags(p.group(1))
                        return self._clean_whitespace(text)
            # å…œåº•ï¼šå…¨å±€ç¬¬ä¸€ä¸ª<p>
            p = re.search(r'<p[^>]*>(.*?)</p>', html_text, re.I | re.S)
            if p:
                text = self._strip_html_tags(p.group(1))
                return self._clean_whitespace(text)
            return None
        except Exception:
            return None

    def _strip_html_tags(self, text):
        text = re.sub(r'<script[\s\S]*?</script>', ' ', text, flags=re.I)
        text = re.sub(r'<style[\s\S]*?</style>', ' ', text, flags=re.I)
        text = re.sub(r'<[^>]+>', ' ', text)
        return html.unescape(text)

    def _clean_whitespace(self, text):
        return re.sub(r'\s+', ' ', (text or '')).strip()
    
    async def crawl_all_pages(self):
        start_time = time.time()
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = []
            for page in range(self.start_page, self.end_page + 1):
                task = self.fetch_page(session, page)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for page, result in enumerate(results, start=self.start_page):
                if isinstance(result, Exception):
                    print(f"ç¬¬ {page} é¡µå‘ç”Ÿå¼‚å¸¸: {result}")
                elif isinstance(result, list):
                    self.all_items.extend(result)
        return self.all_items
    

    async def get_data(self, trigger_time: str) -> pd.DataFrame:
        self.all_items = []  # æ¸…ç©ºç´¯ç§¯çš„æ•°æ®
        
        try:
            items = await self.crawl_all_pages()
        except Exception as e:
            logger.error(f"âŒ Failed to crawl pages: {e}")
            # å³ä½¿çˆ¬å–å¤±è´¥ï¼Œä¹Ÿå°è¯•è¿”å›ç©ºDataFrameè€Œä¸æ˜¯æŠ¥é”™
            logger.info("âš ï¸ Returning empty DataFrame due to crawl failure")
            return pd.DataFrame(columns=['title', 'content', 'pub_time', 'url'])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not items:
            logger.warning("âš ï¸ No items collected from crawling")
            return pd.DataFrame(columns=['title', 'content', 'pub_time', 'url'])
        
        logger.info(f"ğŸ“Š Processing {len(items)} collected items...")
        
        df = pd.DataFrame(items)
        
        # å¤„ç†æ—¶é—´å­—æ®µ
        if not df.empty and 'publish_time' in df.columns:
            df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')
            end_dt = pd.to_datetime(trigger_time, errors='coerce')
            mask = pd.Series(True, index=df.index)
            if not pd.isna(end_dt):
                start_dt = end_dt - pd.Timedelta(days=1)
                mask &= (df['publish_time'] >= start_dt) & (df['publish_time'] < end_dt)
            df = df.loc[mask].reset_index(drop=True)
            df['pub_time'] = df['publish_time'].dt.strftime("%Y-%m-%d %H:%M:%S")
        else:
            df['pub_time'] = ''

        if 'intro' in df.columns:
            df['content'] = df['intro'].apply(lambda x: self._clean_whitespace(self._strip_html_tags(str(x))))
        else:
            df['content'] = ""

        df['pub_time'] = df['pub_time'].fillna('')

        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
        keep_cols = ['title', 'content', 'pub_time', 'url']
        for col in keep_cols:
            if col not in df.columns:
                df[col] = ""

        df = df[keep_cols].copy()
        logger.info(f"get sina news until {trigger_time} success. Total {len(df)} rows")
        return df

if __name__ == "__main__":
    crawler = SinaNewsCrawl(start_page=1, end_page=50)
    df = asyncio.run(crawler.get_data("2025-08-21 15:00:00"))
    print(len(df))
    # try:
    #     output_path = os.path.join(os.path.dirname(__file__), "sina_news_crawl.json")
    #     df.to_json(output_path, orient="records", force_ascii=False, date_format="iso")
    #     print(f"Saved JSON to: {output_path}")
    # except Exception as e:
    #     print(f"Failed to save JSON: {e}")
 