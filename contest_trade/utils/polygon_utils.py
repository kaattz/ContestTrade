"""
polygon çš„å·¥å…·å‡½æ•°
1. è·å–ç¾è‚¡æ–°é—»
"""
import sys
from pathlib import Path
import os
import json
import pandas as pd
import requests
from functools import lru_cache
import hashlib
import pickle
import time
from typing import List
from config.config import cfg

DEFAULT_POLYGON_CACHE_DIR = Path(__file__).parent / "polygon_cache"

class CachedPolygonClient:
    def __init__(self, cache_dir=None, api_key=None):
        if not cache_dir:
            self.cache_dir = DEFAULT_POLYGON_CACHE_DIR
        else:
            self.cache_dir = Path(cache_dir)
        if not self.cache_dir.exists():
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–APIå¯†é’¥
        if not api_key:
            api_key = cfg.polygon_key
        
        self.api_key = api_key
        self.base_url = "https://api.polygon.io/v2"
        self.rate_limit_delay = 0.2  # APIé™åˆ¶ï¼Œæ¯ç§’æœ€å¤š5æ¬¡è¯·æ±‚

    def run(self, endpoint: str, params: dict, verbose: bool = False):
        """
        è¿è¡Œpolygon APIè¯·æ±‚å¹¶ç¼“å­˜ç»“æœ
        
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„
            params: è¯·æ±‚å‚æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        params_str = json.dumps(params, sort_keys=True)
        return self.run_with_cache(endpoint, params_str, verbose)
    
    def run_with_cache(self, endpoint: str, params_str: str, verbose: bool = False):
        params = json.loads(params_str)
        
        # åˆ›å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        endpoint_clean = endpoint.replace('/', '_').replace('?', '_').replace(':', '_').replace('=', '_').replace('&', '_').replace('-', '_').lstrip('_')
        cache_key = f"{endpoint_clean}_{hashlib.md5(params_str.encode()).hexdigest()}"
        endpoint_cache_dir = self.cache_dir / endpoint_clean
        if not endpoint_cache_dir.exists():
            endpoint_cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = endpoint_cache_dir / f"{cache_key}.pkl"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if cache_file.exists():
            if verbose:
                print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½: {cache_file}")
            with open(cache_file, "rb") as f:
                return pickle.load(f)
        else:
            if verbose:
                print(f"ğŸŒ APIè¯·æ±‚: {endpoint} å‚æ•°: {params}")
            
            # é™åˆ¶APIè¯·æ±‚é¢‘ç‡
            time.sleep(self.rate_limit_delay)
            
            try:
                # æ„å»ºå®Œæ•´URL
                url = f"{self.base_url}{endpoint}"
                params['apiKey'] = self.api_key
                # å‘é€è¯·æ±‚
                response = requests.get(url, params=params)
                response.raise_for_status()
                result = response.json()
                # ä¿å­˜åˆ°ç¼“å­˜
                if verbose:
                    print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {cache_file}")
                with open(cache_file, "wb") as f:
                    pickle.dump(result, f)
                
                return result
            except Exception as e:
                if verbose:
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
                raise e


    def get_stock_news(self, symbol: str = None, limit: int = 1000, verbose: bool = False):
        """
        è·å–è‚¡ç¥¨æ–°é—»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç  (å¯é€‰)
            limit: è¿”å›æ–°é—»æ•°é‡
        """
        endpoint = '/reference/news'
        params = {'order': 'desc', 'limit': limit, 'sort': 'published_utc'}
        if symbol:
            params['ticker'] = symbol
        return self.run(endpoint, params, verbose=verbose)


# åˆ›å»ºå…¨å±€ç¼“å­˜å®¢æˆ·ç«¯
polygon_cached = CachedPolygonClient()


@lru_cache(maxsize=1000)
def get_us_stock_news(symbol: str = None, limit: int = 1000, verbose: bool = False):
    """è·å–ç¾è‚¡æ–°é—»"""
    result = polygon_cached.get_stock_news(symbol, limit, verbose)
    df = process_polygon_news(result)
    if df is not None and not df.empty and 'published_utc' in df.columns:
        df['published_utc'] = pd.to_datetime(df['published_utc'])
        df = df.sort_values('published_utc', ascending=False).reset_index(drop=True)
        return df
    return pd.DataFrame()


def process_polygon_news(result: dict):
    "å¤„ç†å“åº”ç»“æœä¸­çš„æ–°é—»"
    if not result:
        print("Fail to get polygon news")
        return pd.DataFrame()

    items = result.get('results') if isinstance(result, dict) else result
    if not items:
        return pd.DataFrame()

    rows = []
    for item in items:
        if not isinstance(item, dict):
            continue
        title = (item.get('title') or '').replace('\n', ' ').strip()
        published = (item.get('published_utc') or '').strip()
        url = (item.get('article_url') or '').strip()
        desc = (item.get('description') or '').replace('\n', ' ').strip()
        rows.append({
            'title': title,
            'published_utc': published,
            'article_url': url,
            'description': desc,
        })
    df = pd.DataFrame(rows, columns=['title', 'published_utc', 'article_url', 'description'])
    return df

if __name__ == "__main__":
    df = get_us_stock_news()
    print(df)
    records_df = df.copy()
    if 'published_utc' in records_df.columns:
        if pd.api.types.is_datetime64_any_dtype(records_df['published_utc']):
            records_df['published_utc'] = records_df['published_utc'].astype(str)  # æˆ– .dt.strftime('%Y-%m-%d %H:%M:%S%z')
    with open("polygon_news.json", "w", encoding="utf-8") as f:
        json.dump(records_df.to_dict(orient='records'), f, ensure_ascii=False, indent=4)